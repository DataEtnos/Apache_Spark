# Apache_Spark
<h1>Introdução ao Spark DataFrames</h1><h2>Visão Geral do Spark</h2><p>O Spark é uma plataforma de processamento de dados em larga escala open source criada pela empresa Databricks. Ele permite trabalhar com conjuntos de dados massivos de forma paralela e distribuída, obtendo alto desempenho.</p><p>Alguns pontos importantes sobre o Spark:</p><ul><li><p>Processamento em memória: O Spark tenta processar os dados na memória, evitando operações de E/S desnecessárias e obtendo performance superior.</p></li><li><p>Tolerância a falhas: O Spark pode recuperar e replicar dados em caso de falhas de hardware.</p></li><li><p>Uso generalizado: Pode ser usado para processamento de batch, streaming, machine learning, SQL e graph processing.</p></li><li><p>Ecossistema: Possui bibliotecas para facilitar tarefas específicas como DataFrames, Spark SQL, Streaming, MLlib e GraphX.</p></li></ul><p>O Spark utiliza uma estrutura de dados chamada RDD (Resilient Distributed Dataset) para representar os dados de forma distribuída entre os nós do cluster. Os RDDs são imutáveis e podem residir na memória, proporcionando performance superior para algoritmos iterativos.</p><h2>Limitações dos RDDs</h2><p>Embora os RDDs sejam a base do Spark, trabalhar diretamente com eles tem algumas desvantagens:</p><ul><li><p>Não possuem esquema: Como os RDDs são coleções genéricas, não carregam informações sobre o tipo e estrutura dos dados. Isso dificulta a validação e tratamento de erros.</p></li><li><p>Sem otimizações: Operações como filtragem e junções precisam ser feitas manualmente pelo programador, sem otimizações automáticas.</p></li><li><p>Código verboso: Trabalhar com RDDs puros requer muito código boilerplate para transformações e ações básicas.</p></li></ul><h2>Introdução aos Spark DataFrames</h2><p>Para resolver essas limitações, o Spark introduziu os DataFrames em versões posteriores. Os DataFrames provêm uma interface de tabela para consultar dados, semelhante ao uso de SQL.</p><p>Os benefícios dos DataFrames:</p><ul><li><p>Possuem esquema: O DataFrame armazena metadados sobre os nomes e tipos das colunas. Isso facilita a aplicação de validações e tratamento de erros.</p></li><li><p>Otimizações automáticas: O Spark cria planos de execução otimizados para operações como filtragem, agregações e junções.</p></li><li><p>Código mais conciso: As operações podem ser encadeadas de forma intuitiva, com menos código boilerplate.</p></li></ul><p>Internamente, os DataFrames são construídos sobre RDDs, mas com informação de esquema adicional. Eles aproveitam as otimizações internas do Spark SQL para melhor performance.</p><h2>Criando uma Sessão Spark</h2><p>Para começar a usar o Spark, precisamos criar uma sessão Spark, que é nosso ponto de entrada principal para trabalhar com a API:</p><pre><code class="language-python">from pyspark.sql import SparkSessionspark = SparkSession.builder \    .appName(&quot;MeuAppSpark&quot;) \    .getOrCreate()</code></pre><p>Alguns detalhes:</p><ul><li>Importamos a classe SparkSession do módulo pyspark.sql</li><li>Usamos o método builder para construir a sessão</li><li>Definimos um nome de aplicativo com .appName()</li><li>E chamamos .getOrCreate() para inicializar a sessão</li></ul><p>A sessão Spark é o ponto de partida para criação de DataFrames, execução de SQL e interação com o cluster Spark.</p><h2>Lendo Arquivos CSV</h2><p>Uma operação comum é a leitura de arquivos CSV para criar um DataFrame. Isso pode ser feito com o método spark.read.csv():</p><pre><code class="language-python">df = spark.read.csv(&quot;dados.csv&quot;)</code></pre><p>Por padrão, alguns detalhes da leitura:</p><ul><li>Cabeçalho: O cabeçalho não é lido automaticamente</li><li>Delimitador: Usa vírgula (,) como separador de campo</li><li>Inferência de tipos: Lê todas as colunas como strings</li></ul><p>Podemos alterar esses comportamentos passando opções adicionais:</p><pre><code class="language-python">df = spark.read.csv(&quot;dados.csv&quot;, header=True, sep=&quot;;&quot;, inferSchema=True)</code></pre><ul><li>header=True: Considera a primeira linha como cabeçalho</li><li>sep=&quot;;&quot;: Define ponto-e-vírgula como separador</li><li>inferSchema=True: Analisa os dados para inferir o tipo de cada coluna</li></ul><h2>Exibindo DataFrames</h2><p>Para visualizar o conteúdo de um DataFrame, podemos usar o método .show():</p><pre><code class="language-python">df.show() </code></pre><p>Isso exibe as primeiras 20 linhas do DataFrame em formato tabular.</p><p>Também podemos inspecionar os metadados do DataFrame com o método .printSchema():</p><pre><code class="language-python">df.printSchema()</code></pre><p>Que imprime os nomes e tipos de dados de cada coluna.</p><h2>Filtrando Linhas</h2><p>Uma operação crucial é filtrar linhas de acordo com algum critério.</p><p>Isso pode ser feito facilmente com o método .filter():</p><pre><code class="language-python">filtrado = df.filter(df.idade &gt; 20)</code></pre><p>O filter aceita uma expressão SQL ou função lambda que retorna verdadeiro ou falso para cada linha.</p><p>Internamente, ele otimiza a filtragem para rodar em paralelo no cluster.</p><h2>Agrupando e Agregando</h2><p>Outra tarefa comum é agrupar os dados por uma coluna categórica e executar agregações:</p><pre><code class="language-python">agrupado = df.groupBy(&quot;departamento&quot;) \             .sum(&quot;salario&quot;) \             .show()</code></pre><p>O grupo é definido pelo groupBy(), enquanto as agregações como soma ou contagem são especificadas em métodos como .sum() e .count().</p><p>Novamente, o Spark otimiza essas agregações para performance ideal.</p><h2>Juntando DataFrames</h2><p>Podemos também juntar dois DataFrames, similar a junções SQL:</p><pre><code class="language-python">juntado = df1.join(df2, df1.colunaA == df2.colunaB)</code></pre><p>Especificamos a condição de junção comparando as colunas desejadas de cada DataFrame.</p><p>Vários tipos de junção são suportados como inner join, outer join, left semi join etc.</p><h2>Salvando Resultados</h2><p>Por fim, depois de processar seus dados, você pode salvar o resultado em um novo arquivo:</p><pre><code class="language-python">df.write.csv(&quot;saida.csv&quot;) </code></pre><p>Diversos formatos são aceitos como CSV, JSON, Parquet e outros.</p><h2>Conclusão</h2><p>Os DataFrames do Spark provêm uma interface de alto nível para processamento distribuído de dados em larga escala.</p><p>Comparado a usar RDDs diretamente, os DataFrames são mais fáceis de usar, mais concisos e se beneficiam de otimizações automáticas do Spark SQL.</p><p>Esperamos que este guia tenha sido útil para você começar a manipular seus dados com PySpark!</p>
